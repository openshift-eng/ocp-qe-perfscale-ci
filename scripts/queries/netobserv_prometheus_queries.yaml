###############
# KPI metrics #
###############

# total flows processed
- query: sum(netobserv_ingest_flows_processed)
  metricName: nFlowsProcessedTotals

# total flows processed per minute
- query: sum(rate(netobserv_ingest_flows_processed[1m])*60)
  metricName: nFlowsProcessedPerMinuteTotals

# total flows processed from workload namespaces
- query: sum(netobserv_namespace_flows_total{SrcK8S_Namespace=~"node-density-heavy.*|ingress-perf|cluster-density.*"})
  metricName: nWorkloadFlowsProcessedTotals

# total flows processed from workload namespaces per minute
- query: sum(rate(netobserv_namespace_flows_total{SrcK8S_Namespace=~"node-density-heavy.*|ingress-perf|cluster-density.*"}[1m])*60)
  metricName: nWorkloadFlowsProcessedPerMinuteTotals

# total bytes processed from workload namespaces by netobserv
- query: sum(rate(netobserv_workload_ingress_bytes_total{DstK8S_Namespace=~"node-density-heavy.*|ingress-perf|cluster-density.*",DstK8S_Type!="Service"}[1m])*60)
  metricName: nWorkloadBytesProcessedPerMinuteNetobserv

# total bytes processed from workload namespaces by cadvisor / other means
- query: sum(rate(container_network_receive_bytes_total{job="kubelet", metrics_path="/metrics/cadvisor", cluster="", namespace=~"node-density-heavy.*|ingress-perf|cluster-density.*"}[1m])*60)
  metricName: nWorkloadBytesProcessedPerMinuteCadvisor

# total flows errored
- query:
    (sum(increase(netobserv_agent_errors_total[1m])) OR on() vector(0))
    + (sum(increase(netobserv_ingest_errors[1m])) OR on() vector(0))
    + (sum(increase(netobserv_encode_prom_errors[1m])) OR on() vector(0))
    + (sum(increase(netobserv_loki_batch_retries_total[1m])) OR on() vector(0))
  metricName: nFlowsErroredTotals

# total flows errored per minute
- query:
    (sum(rate(netobserv_agent_errors_total[1m])*60) OR on() vector(0))
    + (sum(rate(netobserv_ingest_errors[1m])*60) OR on() vector(0))
    + (sum(rate(netobserv_encode_prom_errors[1m])*60) OR on() vector(0))
    + (sum(rate(netobserv_loki_batch_retries_total[1m])*60) OR on() vector(0))
  metricName: nFlowsErroredPerMinuteTotals

# flows processed by ingester per minute aggregated by FLP pods
- query: sum(rate(netobserv_ingest_flows_processed[1m])*60) by (pod)
  metricName: nFlowsProcessedPerMinute

# flows errored by ingester per minute aggregated by FLP pods
- query: sum(rate(netobserv_ingest_errors[1m])*60) by (pod)
  metricName: nFlowsErroredPerMinute

#####################
# NetObserv metrics #
#####################

# netobserv pods CPU usage
- query: pod:container_cpu_usage:sum{namespace=~"netobserv|netobserv-privileged|openshift-netobserv-operator"}
  metricName: cpuUsage

# netobserv pods memory usage (RSS) - eBPF is omitted here as that is collected below
- query: sum(container_memory_rss{namespace=~"netobserv|openshift-netobserv-operator", container=""}) by (container, pod, namespace, node)
  metricName: memoryUsageRSS

# netobserv pods memory usage (Working Set) - eBPF is omitted here as that is collected below
- query: sum(container_memory_working_set_bytes{namespace=~"netobserv|openshift-netobserv-operator", container=""}) by (container, pod, namespace, node)
  metricName: memoryUsageWorkingSet

################
# eBPF metrics #
################

# total CPU usage by eBPF
- query: sum(pod:container_cpu_usage:sum{namespace="netobserv-privileged"})
  metricName: cpuEBPFTotals

# total RSS by all eBPF pods
- query: sum(container_memory_rss{namespace="netobserv-privileged", container=""})
  metricName: rssEBPFTotals

# total working set memory by all eBPF pods
- query: sum(container_memory_working_set_bytes{namespace="netobserv-privileged", container=""})
  metricName: workingsetEBPFTotals

# eBPF pods memory usage (RSS)
- query: sum(container_memory_rss{namespace="netobserv-privileged", container=""}) by (container, pod, namespace, node)
  metricName: ebpfMemoryUsageRSS

# eBPF pods memory usage (Working Set)
- query: sum(container_memory_working_set_bytes{namespace="netobserv-privileged", container=""}) by (container, pod, namespace, node)
  metricName: ebpfMemoryUsageWorkingSet

# eBPF drops
- query: sum(rate(netobserv_agent_dropped_flows_total[1m]))
  metricName: ebpfFlowsDroppedTotals

# ebpf Ring buffer/Map Ratio
- query: (sum(rate(netobserv_agent_evicted_flows_total{source="accounter"}[1m])) OR on() vector(0)) / sum(rate(netobserv_agent_evicted_flows_total{source="hashmap"}[1m]))
  metricName: ebpfRingBufferMapRatioTotals

###############
# FLP metrics #
###############

# total CPU usage by FLP - per pod is not included here as that is collected above
- query: sum(pod:container_cpu_usage:sum{namespace="netobserv", pod=~"flowlogs.*"})
  metricName: cpuFLPTotals

# total RSS by all FLP pods - per pod is not included here as that is collected above
- query: sum(container_memory_rss{namespace="netobserv", container="", pod=~"flowlogs.*"})
  metricName: rssFLPTotals

# total working set memory by all FLP pods - per pod is not included here as that is collected above
- query: sum(container_memory_working_set_bytes{namespace="netobserv", container="", pod=~"flowlogs.*"})
  metricName: workingsetFLPTotals

################
# Loki metrics #
################

# total CPU usage by all loki pods - per pod is not included here as that is collected above
- query: sum(pod:container_cpu_usage:sum{namespace="netobserv", pod=~"loki.*"})
  metricName: cpuLokiTotals

# total RSS by all loki pods - per pod is not included here as that is collected above
- query: sum(container_memory_rss{namespace="netobserv", pod=~"loki.*", container=""})
  metricName: rssLokiTotals

# total working set memory by all loki pods - per pod is not included here as that is collected above
- query: sum(container_memory_working_set_bytes{namespace="netobserv", pod=~"loki.*", container=""})
  metricName: workingsetLokiTotals

# lokistack PVC usage averaging over 5 mins period
- query: sum by (persistentvolumeclaim) (avg_over_time(kubelet_volume_stats_used_bytes{namespace="netobserv", persistentvolumeclaim=~".*loki.*"}[5m]))
  metricName: lokiStorageUsage

# total loki records written
- query: sum(netobserv_loki_sent_entries_total)
  metricName: lokiRecordsWritten

# loki records written per minute
- query: sum(rate(netobserv_loki_sent_entries_total[1m])*60)
  metricName: lokiRecordsWrittenPerMinute

# total records dropped while sending to Loki (inverse of netobserv_loki_sent_entries_total)
- query: sum(netobserv_loki_dropped_entries_total)
  metricName: lokiRecordsDropped

# total records dropped while sending to Loki per minute (inverse of netobserv_loki_sent_entries_total)
- query: sum(rate(netobserv_loki_dropped_entries_total[1m])*60)
  metricName: lokiRecordsDroppedPerMinute

#################
# Kafka metrics #
#################

# Kafka PVC usage averaging over 5 mins period
- query: sum by (persistentvolumeclaim) (avg_over_time(kubelet_volume_stats_used_bytes{namespace="netobserv", persistentvolumeclaim=~".*kafka.*"}[5m]))
  metricName: kafkaStorageUsage

# total CPU usage by all kafka pods - per pod is not included here as that is collected above
- query: sum(pod:container_cpu_usage:sum{namespace="netobserv", pod=~"kafka.*"})
  metricName: cpuKafkaTotals

# total RSS by all kafka pods - per pod is not included here as that is collected above
- query: sum(container_memory_rss{namespace="netobserv", container="", pod=~"kafka.*"})
  metricName: rssKafkaTotals

# total working set memory by all Kafka pods - per pod is not included here as that is collected above
- query: sum(container_memory_working_set_bytes{namespace="netobserv", container="", pod=~"kafka.*"})
  metricName: workingsetKafkaTotals

###########################
# Total metrics #
###########################

# Total NetObserv CPU Usage, assuming Loki and Kafka also runs in netobserv NS
- query: (sum(pod:container_cpu_usage:sum{namespace=~".*netobserv.*"}) or on() vector(0))
  metricName: TotalNetObservCPUUsage

# Total NetObserv RSS Usage, assuming Loki and Kafka also runs in netobserv NS
- query:  (sum(container_memory_rss{namespace=~".*netobserv.*", container=""}) or on() vector(0))
  metricName: TotalNetObservRSSUsage

# Total Cluster CPU Usage
- query:  sum(label_replace(cluster:cpu_usage_cores:sum{prometheus="openshift-monitoring/k8s"}, "resource", "cluster_cpu_usage", "prometheus", ".*")) by (resource)
  metricName: TotalClusterCPUUsage

# Total Cluster Memory Usage
- query:  sum(label_replace(cluster:memory_usage_bytes:sum{prometheus="openshift-monitoring/k8s"}, "resource", "cluster_memory_usage", "prometheus", ".*")) by (resource)
  metricName: TotalClusterRSSUsage

# Total Network Transmit bytes/sec
- query:  sum(instance:node_network_transmit_bytes_excluding_lo:rate1m * on(instance) group_left(role) (label_replace(max by (node) (kube_node_role{role=~".+"}), "instance", "$1", "node", "(.*)")))
  metricName: TotalNetworkBytesOut

# Total Network Receive bytes/sec
- query:  sum(instance:node_network_receive_bytes_excluding_lo:rate1m * on(instance) group_left(role) (label_replace(max by (node) (kube_node_role{role=~".+"}), "instance", "$1", "node", "(.*)")))
  metricName: TotalNetworkBytesIn


# NetObserv % CPU usage as a factor total cluster CPU usage
- query: sum(label_replace((((sum(namespace:container_cpu_usage:sum{namespace=~".*netobserv.*"})) / on() group_right() cluster:cpu_usage_cores:sum)*100), "resource", "cpu_usage%", "prometheus", ".*")) by (resource)
  metricName: NetObservClusterCPUPercentUtil

# NetObserv % memory usage as a factor total cluster memory usage
- query: sum(label_replace((sum(container_memory_usage_bytes{namespace=~".*netobserv.*",  container=""}) / on() group_right() cluster:memory_usage_bytes:sum)*100, "resource", "memory_usage%", "prometheus", ".*")) by (resource)
  metricName: NetObservClusterMemPercentUtil
